{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier as sklDecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = clf.fit(X, y)\n",
    "# iris.keys()\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_wine\n",
    "# wine = load_wine()\n",
    "# X, y = wine.data, wine.target\n",
    "# X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.8888888888888888\n",
      "0.9924812030075187 0.9555555555555556\n",
      "1.0 0.8888888888888888\n",
      "1.0 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from tree.decision_tree import DecisionTree\n",
    "clf = DecisionTree(max_depth=5, criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "clf = DecisionTree(max_depth=5, criterion='gini', extreme_random=True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "clf = sklDecisionTreeClassifier(max_depth=5, criterion='gini')\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestBase:\n",
    "    \"\"\"\n",
    "    A Random Forest Classifier Machine Learning algorithm.\n",
    "\n",
    "    Parameters:\n",
    "        n_estimators (int): The number of decision trees to use\n",
    "        max_tree_depth (int): The maximum depth of the component decision tree\n",
    "        criterion (str): The criterion for splitting - 'gini' or 'entropy'\n",
    "        max_features_split (float): The maximum number of features to consider for making a decision tree split (default=number of features)\n",
    "        extreme_random (bool): Whether to use randomly generated thresholds for splitting (default=False)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        n_estimators: int=100, \n",
    "        max_tree_depth: int=20, \n",
    "        criterion: str='gini',\n",
    "        max_features_split: int=None,\n",
    "        extreme_random: bool=False\n",
    "        ):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.criterion = criterion\n",
    "        self.max_features_split = max_features_split\n",
    "        self.extreme_random = extreme_random\n",
    "\n",
    "\n",
    "    def generate_random_decision_trees(self, X, y, n_estimators, max_features_split, random_state=100):\n",
    "        print(n_estimators)\n",
    "        estimators = []\n",
    "        for _ in range(n_estimators):\n",
    "            features_to_skip = []\n",
    "            if self.max_features_split is not None:\n",
    "                # print(self.n_features, self.n_features-self.max_features_split)\n",
    "                features_to_skip = list(np.random.choice(self.n_features, self.n_features-self.max_features_split, replace=False))\n",
    "                # print(features_to_skip)\n",
    "\n",
    "            clf = DecisionTree(\n",
    "                max_depth=self.max_tree_depth, \n",
    "                criterion=self.criterion, \n",
    "                features_to_skip=features_to_skip,\n",
    "                extreme_random=self.extreme_random\n",
    "                )\n",
    "            random_indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
    "            X_sample = X[random_indices]\n",
    "            y_sample = y[random_indices]\n",
    "            clf.fit(X_sample, y_sample)\n",
    "            estimators.append(clf)\n",
    "        return estimators\n",
    "\n",
    "\n",
    "\n",
    "class RandomForestClassifier(RandomForestBase):\n",
    "    def __init__(self, \n",
    "        n_estimators: int=100, \n",
    "        max_tree_depth: int=20, \n",
    "        criterion: str='gini',\n",
    "        max_features_split: int=None,\n",
    "        extreme_random: bool=False\n",
    "        ):\n",
    "        super().__init__(n_estimators=n_estimators, max_tree_depth=max_tree_depth, \n",
    "            criterion=criterion, max_features_split=max_features_split,\n",
    "            extreme_random=extreme_random\n",
    "            )\n",
    "\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        n_samples, n_features = X.shape\n",
    "        assert n_samples == y.shape[0]\n",
    "        assert len(y.shape) == 1 or y.shape[1] == 1\n",
    "\n",
    "        self.n_samples, self.n_features = n_samples, n_features\n",
    "\n",
    "        self.num_classes = len(Counter(y))\n",
    "\n",
    "        self.estimators = self.generate_random_decision_trees(X, y, self.n_estimators, self.max_features_split)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_preds = [clf.predict(X) for clf in self.estimators]\n",
    "        y_preds = [np.squeeze(np.eye(self.num_classes)[a.reshape(-1)]) for a in y_preds]\n",
    "        y_pred_proba = np.array(y_preds).mean(axis=0)\n",
    "\n",
    "        return y_pred_proba\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred_proba = self.predict_proba(X)\n",
    "        y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class RandomForestRegressor(RandomForestBase):\n",
    "    def __init__(self, \n",
    "        n_estimators: int=100, \n",
    "        max_tree_depth: int=20, \n",
    "        criterion: str='gini',\n",
    "        max_features_split: int=None,\n",
    "        extreme_random: bool=False\n",
    "        ):\n",
    "        super().__init__(n_estimators=n_estimators, max_tree_depth=max_tree_depth, \n",
    "            criterion=criterion, max_features_split=max_features_split,\n",
    "            extreme_random=extreme_random\n",
    "            )\n",
    "\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        n_samples, n_features = X.shape\n",
    "        assert n_samples == y.shape[0]\n",
    "        assert len(y.shape) == 1 or y.shape[1] == 1\n",
    "\n",
    "        self.n_samples, self.n_features = n_samples, n_features\n",
    "\n",
    "        self.estimators = self.generate_random_decision_trees(X, y, self.n_estimators, self.max_features_split)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_preds = np.array([clf.predict(X) for clf in self.estimators]).mean(axis=0)\n",
    "\n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "1.0 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, criterion='gini', extreme_random=False, max_features_split=8)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor as sklRandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor as sklDecisionTreeRegressor\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=10)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4814179240098624 3256.690470682478 0.0\n"
     ]
    }
   ],
   "source": [
    "clf = sklDecisionTreeRegressor(max_depth=10)\n",
    "clf = sklRandomForestRegressor(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "# accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\n",
    "print(r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred), r2_score(y_test, np.ones(y_test.shape)*np.mean(y_test)))\n",
    "# dot_data = clf.export_graphviz(feature_names=data.feature_names, regression=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tvpc00032/miniconda3/envs/ml_scratch/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tvpc00032/miniconda3/envs/ml_scratch/lib/python3.11/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/tvpc00032/Code/projects/ml-from-scratch/packages/tree/tree/decision_tree.py:49: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  variance = y.var()\n",
      "/home/tvpc00032/miniconda3/envs/ml_scratch/lib/python3.11/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/home/tvpc00032/miniconda3/envs/ml_scratch/lib/python3.11/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46733009232291556 3345.1619187553015 0.0\n"
     ]
    }
   ],
   "source": [
    "# clf = DecisionTree(max_depth=10, criterion='variance')\n",
    "clf = RandomForestRegressor(n_estimators = 100, criterion='variance', max_tree_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "# accuracy_score(y_train, y_train_pred), accuracy_score(y_test, y_test_pred)\n",
    "print(r2_score(y_test, y_test_pred), mean_squared_error(y_test, y_test_pred), r2_score(y_test, np.ones(y_test.shape)*np.mean(y_test)))\n",
    "# dot_data = clf.export_graphviz(feature_names=data.feature_names, regression=True)\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d41c41a09a0f5a2ceecde79176bce900216b18a9fc439559abf44e262b257dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
